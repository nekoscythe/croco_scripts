#!/bin/bash

get_root_dir() {
    local dir="$(pwd)"
    while [[ ! -f "$dir/settings.yaml" && "$dir" != "/" ]]; do
        dir=$(dirname "$dir")
    done

    [[ -f "$dir/settings.yaml" ]] || error "Root directory not found."
    echo "$dir"
}


# Ensure the script is run from the test directory
TEST_DIR="$(pwd)"
ROOT_DIR=$(get_root_dir)
METADATA_FILE="$TEST_DIR/metadata.yaml"
SETTINGS_FILE="$ROOT_DIR/settings.yaml"
LOG_FILE="$TEST_DIR/outputs/run_test.log"
OUTPUTS_DIR="$TEST_DIR/outputs"
ARCHIVE_DIR="$TEST_DIR/outputs/archive"

echo "TEST_DIR: $TEST_DIR"

# Check if settings.yaml exists
if [[ ! -f "$SETTINGS_FILE" ]]; then
    echo "Error: settings.yaml not found in $ROOT_DIR."
    exit 1
fi

# Check if metadata.yaml exists
if [[ ! -f "$METADATA_FILE" ]]; then
    echo "Error: metadata.yaml not found in $TEST_DIR."
    exit 1
fi

# Create necessary directories
mkdir -p "$OUTPUTS_DIR"
mkdir -p "$ARCHIVE_DIR"

# renew the log file in case it already exists
if [[ -f "$LOG_FILE" ]]; then
    rm "$LOG_FILE"
fi
# Redirect output to the log file
exec > >(tee "$LOG_FILE") 2>&1

# Read test details from metadata.yaml
TEST_NAME=$(yq eval '.test_name' "$METADATA_FILE")
TEST_ID=$(yq eval '.test_id' "$METADATA_FILE")
TEST_REASON=$(yq eval '.reason' "$METADATA_FILE")
BINARY_PATH=$(yq eval '.binary_path' "$METADATA_FILE")

# Convert paths to relative format
REL_INPUT_FILE="inputs/infile.in"

# Check if the binary exists
if [[ ! -f "$BINARY_PATH" ]]; then
    echo "Error: Binary not found at $BINARY_PATH."
    exit 1
fi

# Read the companion file to get the stored dependency hashes
COMPANION_FILE="$BINARY_PATH.hashes"
if [[ ! -f "$COMPANION_FILE" ]]; then
    echo "Error: Companion file not found for binary at $COMPANION_FILE."
    exit 1
fi

# Get the hashes stored
declare -A STORED_HASHES
# Read the companion file line by line
while IFS= read -r line; do
    # Split the line by space into separate filename:hash pairs
    for pair in $line; do
        # Split each pair at the colon ':'
        file=$(echo "$pair" | cut -d ':' -f1)
        hash=$(echo "$pair" | cut -d ':' -f2)
        
        # Store the file and hash in the associative array
        STORED_HASHES["$file"]="$hash"
    done
done < "$COMPANION_FILE"


# Read the dependencies from settings.yaml

DEPENDENCY_LOCATIONS=($(yq eval-all 'explode(.) | .dependencies[].location' "$SETTINGS_FILE"))
DEPENDENCY_PATHS=($(yq eval-all 'explode(.) | .dependencies[].path' "$SETTINGS_FILE"))
DEPENDENCY_COUNT=${#DEPENDENCY_LOCATIONS[@]}

DEPENCIES_MATCH=true
# Iterate over the dependencies and combine location and path
for ((i = 0; i < DEPENDENCY_COUNT; i++)); do
    # Combine the location and path
    location="${DEPENDENCY_LOCATIONS[$i]}"
    path="${DEPENDENCY_PATHS[$i]}"
    full_path="${location}/${path}"

    # compare the hashes
    hash=$(sha256sum "$full_path" | awk '{print $1}')
    # compare the hash with the stored hash
    stored_hash=${STORED_HASHES["$path"]}
    if [[ -z "$stored_hash" ]]; then
        echo "No stored hash found for $path"
        DEPENCIES_MATCH=false
    elif [[ "$hash" == "$stored_hash" ]]; then
        echo "✔ Match: $path"
    else
        echo "❌ Mismatch: $path"
        echo "   Expected: $stored_hash"
        echo "   Found:    $hash"
        DEPENCIES_MATCH=false
    fi
done

if [[ "$DEPENCIES_MATCH" == false ]]; then
    echo "Error: Dependency hashes do not match. Please recompile the binary."
    exit 1
fi



# Confirm test details
echo "Test Details:"
echo "-------------"
echo "Test Name: $TEST_NAME"
echo "Test ID: $TEST_ID"
echo "Reason: $TEST_REASON"
echo "Binary Path: $BINARY_PATH"
echo "Input File: $REL_INPUT_FILE"
echo "Log File: $LOG_FILE"
echo "Archive Directory: $ARCHIVE_DIR"
echo ""

# Confirm if the user wants to proceed
read -p "Do you want to run this test? (y/n): " CONFIRM
if [[ ! "$CONFIRM" =~ ^[Yy]$ ]]; then
    echo "Test execution aborted."
    exit 0
fi

# Ask for parallelization method
echo "Select parallel execution mode:"
echo "1) OpenMP (OMP_NUM_THREADS)"
echo "2) MPI (mpirun)"
echo "3) MPI with SLURM (srun)"
read -p "Enter choice (1-3): " PARALLEL_MODE

get_num_cores() {
    AVAILABLE_CORES=$(nproc --all)
    echo "Number of available cores: $AVAILABLE_CORES"
    read -p "Enter the number of cores to use (1-$AVAILABLE_CORES): " NUM_CORES

    if [[ ! "$NUM_CORES" =~ ^[0-9]+$ ]] || (( NUM_CORES < 1 )) || (( NUM_CORES > AVAILABLE_CORES )); then
        echo "Error: Invalid number of cores. Please enter a number between 1 and $AVAILABLE_CORES."
        exit 1
    fi
    echo "$NUM_CORES"
}


get_slurm_walltime() {
    read -p "Enter the number of hours to run the job (1-168): " WALLTIME_HOURS
    #check if the input is in the range
    if [[ ! "$WALLTIME_HOURS" =~ ^[0-9]+$ ]] || (( WALLTIME_HOURS < 1 )) || (( WALLTIME_HOURS > 168 )); then
        echo "Error: Invalid number of hours. Please enter a number between 1 and 168."
        exit 1
    fi
    echo "$WALLTIME_HOURS"
}

clean_outputs() {
    # Remove all files in the outputs directory while ignoring errors
    rm -f "$OUTPUTS_DIR/*" 2>/dev/null
    # also clean the archive directory while ignoring errors
    rm -f "$ARCHIVE_DIR/*" 2>/dev/null
}

create_archive() {
    # Copy infile and dependencies to the archive directory
    cp "$REL_INPUT_FILE" "$ARCHIVE_DIR"
    mkdir -p "$ARCHIVE_DIR/dependencies"
    for ((i = 0; i < DEPENDENCY_COUNT; i++)); do
        location="${DEPENDENCY_LOCATIONS[$i]}"
        path="${DEPENDENCY_PATHS[$i]}"
        full_path="${location}/${path}"
        cp "$full_path" "$ARCHIVE_DIR/dependencies"
    done
    # Copy the binary and its companion file to the archive directory
    cp "$BINARY_PATH" "$ARCHIVE_DIR"
    cp "$COMPANION_FILE" "$ARCHIVE_DIR"
}



# Read the contents of infile to be added as part of each run log
INPUT_FILE_CONTENTS=$(cat "$REL_INPUT_FILE")



# Add separator and input file contents to the log
echo "============================================" >> "$LOG_FILE"
echo "Run started for test: $TEST_NAME" >> "$LOG_FILE"
echo "Input File: $REL_INPUT_FILE" >> "$LOG_FILE"
echo "============================================" >> "$LOG_FILE"
echo "$INPUT_FILE_CONTENTS" >> "$LOG_FILE"
echo "============================================" >> "$LOG_FILE"


# Execute based on chosen parallelization framework
case $PARALLEL_MODE in
    1)
        NUM_CORES=$(get_num_cores)
        export OMP_NUM_THREADS="$NUM_CORES"
        echo "Running test with OMP_NUM_THREADS=$OMP_NUM_THREADS..."
        # clean outputs first
        clean_outputs
        # archive the files
        create_archive
        # run the test
        "$BINARY_PATH" "$REL_INPUT_FILE"
        ;;
    2)
        # Get number of cores for MPI from metadata.yaml
        NUM_CORES=$(yq eval '.Config.cpu_cores' "$METADATA_FILE")
        echo "Running test with MPI using mpirun ($NUM_CORES processes)..."
        # clean outputs first
        clean_outputs
        # archive the files
        create_archive
        # run the test
        mpirun -n "$NUM_CORES" "$BINARY_PATH" "$REL_INPUT_FILE"
        ;;
    3)
        NUM_CORES=$(yq eval '.Config.cpu_cores' "$METADATA_FILE")
        NUM_NODES=$((NUM_CORES / 128))
        NUM_HOURS=$(get_slurm_walltime)
        echo "NUM_CORES: $NUM_CORES"
        echo "NUM_NODES: $NUM_NODES"
        echo "NUM_HOURS: $NUM_HOURS"
        echo "Running test with MPI using SLURM srun ($NUM_CORES processes)..."

        #clean outputs first
        clean_outputs
        # archive the files
        create_archive
        # run the test

        #create the job script
        JOB_SCRIPT="$ARCHIVE_DIR/job_$TEST_ID.job"
        echo "#!/bin/bash" > "$JOB_SCRIPT"
        echo "#SBATCH --ntasks=$NUM_CORES" >> "$JOB_SCRIPT"
        echo "#SBATCH --nodes=$NUM_NODES" >> "$JOB_SCRIPT"
        echo "#SBATCH --time=$NUM_HOURS:00:00" >> "$JOB_SCRIPT"
        echo "#SBATCH --output=$ARCHIVE_DIR/slurm-%j.out" >> "$JOB_SCRIPT"
        echo "#SBATCH --error=$ARCHIVE_DIR/slurm-%j.err" >> "$JOB_SCRIPT"
        echo "# **** Put all #SBATCH directives above this line! ****" >> "$JOB_SCRIPT"
        echo "" >> "$JOB_SCRIPT"
        echo "# **** Actual commands start here ****" >> "$JOB_SCRIPT"
        echo "module purge" >> "$JOB_SCRIPT"
        echo "module load gcc/9.2.0" >> "$JOB_SCRIPT"
        echo "module load openmpi/4.1.1rc1" >> "$JOB_SCRIPT"
        echo "module load netcdf-fortran/4.6.1" >> "$JOB_SCRIPT"
        echo "module load netcdf-c/4.9.0" >> "$JOB_SCRIPT"
        echo "#Run command" >> "$JOB_SCRIPT"
        echo "srun $BINARY_PATH $REL_INPUT_FILE | tee -a outputs/run_test.log" >> "$JOB_SCRIPT"
        #submit the job
        sbatch "$JOB_SCRIPT"

        ;;
    *)
        echo "Error: Invalid selection."
        exit 1
        ;;
esac

# Check exit status
EXIT_STATUS=$?
if [[ $EXIT_STATUS -ne 0 ]]; then
    echo "Error: Test execution failed with exit code $EXIT_STATUS."
    exit $EXIT_STATUS
fi

echo "Test executed successfully."
